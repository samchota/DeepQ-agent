{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import collections\n",
    "import random\n",
    "import time\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import csv\n",
    "from gym import envs\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import BatchNormalization, Reshape, UpSampling2D, Conv2DTranspose, LeakyReLU, ZeroPadding2D, Input\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras import regularizers, optimizers\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import regularizers, optimizers\n",
    "from keras.utils import to_categorical\n",
    "np.random.seed(1234)\n",
    "import scipy.io as sio\n",
    "import scipy as scp\n",
    "from scipy import stats\n",
    "import operator\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from IPython.display import display, clear_output\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "#config=tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction=0.5\n",
    "#set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_envs = envs.registry.all()\n",
    "env_ids = [env_spec.id for env_spec in all_envs]\n",
    "print(np.transpose(env_ids)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FF\n",
    "class DQNAgent:\n",
    "    def __init__(self,a):\n",
    "        #(210,160,3),6\n",
    "        self.drop = 0\n",
    "        self.state_size = (128,)\n",
    "        self.action_size = 4 # 9 for pacman, 6 for space invader\n",
    "        self.memory = collections.deque(maxlen=1000000)\n",
    "        self.gamma = 0.99    # discount rate 0.95\n",
    "        self.epsilon = 1  # exploration rate 1 \n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.9998\n",
    "        self.learning_rate = 10**-3\n",
    "        self.time_penalty = 0\n",
    "        self.model = self._build_model()\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(512, input_shape=self.state_size, activation='relu'))\n",
    "        #model.add(Dropout(self.drop))\n",
    "        \n",
    "        #model.add(Flatten())\n",
    "        \n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        #model.add(Dropout(self.drop))\n",
    "        \n",
    "        #model.add(Dense(32, activation='relu'))\n",
    "        #model.add(Dropout(self.drop))\n",
    "        \n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary()\n",
    "        return model\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) #previous state, action, reward after action\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            act_values = np.zeros((1,self.action_size))\n",
    "            return act_values,env.action_space.sample()\n",
    "        state = np.expand_dims(state,axis=0)\n",
    "        #print(state.shape)\n",
    "        act_values = self.model.predict(state)\n",
    "        #return np.argmax(act_values[0])  # returns action\n",
    "        return act_values,np.argmax(act_values)   # returns action\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = np.expand_dims(state,axis=0)\n",
    "            next_state = np.expand_dims(next_state,axis=0)\n",
    "            target = reward #if we are done the final reward is our target\n",
    "            if not done: #if we are not done the target is the current reward plut the predicted reward from the next state\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)) #two frames ahead\n",
    "                #print(self.gamma * np.amax(self.model.predict(next_state)))\n",
    "            target_f = self.model.predict(state) \n",
    "            #print(target_f)\n",
    "            target_f[0][action] = target- self.time_penalty\n",
    "            #print(target_f)\n",
    "            print(state.shape)\n",
    "            print(target_f.shape)\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make('SpaceInvaders-v0')\n",
    "env = gym.make('Breakout-ramDeterministic-v0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balls = np.zeros(1000)\n",
    "for i in  range(1000):\n",
    "    balls[i] = env.action_space.sample()\n",
    "np.max(balls)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ale.lives': 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "next_state, reward, done, info = env.step(1)\n",
    "lives_before = info['ale.lives']\n",
    "print(info)\n",
    "#env.render()\n",
    "#plt.imshow(state)\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 132,228\n",
      "Trainable params: 132,228\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(env)\n",
    "episodes = 10000\n",
    "death_penalty = 40\n",
    "agent.epsilon_decay = 0.9996\n",
    "\n",
    "\n",
    "# initialize gym environment and the agent\n",
    "env = gym.make('Breakout-ramDeterministic-v0')\n",
    "#env = gym.make('SpaceInvaders-v0')\n",
    "total_reward_counter = np.empty([episodes])\n",
    "Q_reward_counter = np.empty([episodes])\n",
    "reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1/10000, score: 1.0, epsilon: 1\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "episode: 2/10000, score: 0.0, epsilon: 0.9996\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "episode: 3/10000, score: 1.0, epsilon: 0.9992001600000001\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "episode: 4/10000, score: 0.0, epsilon: 0.9988004799360002\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "episode: 5/10000, score: 2.0, epsilon: 0.9984009597440259\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "episode: 6/10000, score: 2.0, epsilon: 0.9980015993601283\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "episode: 7/10000, score: 0.0, epsilon: 0.9976023987203844\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n",
      "(1, 4)\n",
      "(1, 128)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-795f88598cb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# train the agent with the experience of the episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-6a13865fb818>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m#print(target_f)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/samretrogames/lib/python3.6/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    364\u001b[0m                 parent=self.parent_header, ident=self.topic)\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'I/O operation on closed file'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#agent.epsilon = 1\n",
    "# Iterate the game\n",
    "for e in range(episodes):\n",
    "    total_reward = 0\n",
    "    \n",
    "    # reset state in the beginning of each game\n",
    "    state = env.reset()\n",
    "    state, reward, done, info = env.step(0)\n",
    "    #state = scp.stats.zscore(state)\n",
    "    lives_before = info['ale.lives']\n",
    "    #state = state[:,:,0]\n",
    "    \n",
    "\n",
    "    #state = np.reshape(state, [1, 4])\n",
    "    # time_t represents each frame of the game\n",
    "    # Our goal is to keep the pole upright as long as possible until score of 500\n",
    "    # the more time_t the more score\n",
    "    for time_t in range(1000):\n",
    "        reward_cum = 0\n",
    "        # turn this on if you want to render\n",
    "        #env.render()\n",
    "        #time.sleep(0.01)\n",
    "        # Decide action\n",
    "        #state = np.expand_dims(state,axis=0)\n",
    "        \n",
    "        actvals, action = agent.act(state)\n",
    "        #print(actvals)\n",
    "        #print(action)\n",
    "        #action = np.argmax(actvals)\n",
    "   \n",
    "        # Advance the game to the next frame based on the action.\n",
    "        # Reward is 1 for every frame the pole survived\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        #next_state = scp.stats.zscore(next_state)\n",
    "        #if reward>0:\n",
    "        #    reward = 1\n",
    "        reward_cum += reward\n",
    "        \n",
    "        #next_state = np.expand_dims(next_state,axis=1)\n",
    "        \n",
    "\n",
    "        #next_state = next_state[:,:,0]\n",
    "        \n",
    "        #print(next_state.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "        lives_after = info['ale.lives']\n",
    "        \n",
    "        #if lives_after < lives_before:\n",
    "        #    reward_cum = reward_cum -death_penalty\n",
    "            \n",
    "        total_reward = total_reward + reward_cum    \n",
    "        #next_state = np.reshape(next_state, [1, 4])\n",
    "        # Remember the previous state, action, reward, and done\n",
    "        agent.remember(state, action, reward_cum, next_state, done)\n",
    "        lives_before = lives_after\n",
    "        # make next_state the new current state for the next frame.\n",
    "        state = next_state\n",
    "        # done becomes True when the game ends\n",
    "        # ex) The agent drops the pole\n",
    "        if done:\n",
    "            # print the score and break out of the loop\n",
    "            #clear_output(wait=True)\n",
    "            print(\"episode: {}/{}, score: {}, epsilon: {}\".format(e+1, episodes, total_reward, agent.epsilon))\n",
    "            #print(state.shape)\n",
    "            #print(state.shape)\n",
    "            #plt.plot(state)\n",
    "            #state = np.expand_dims(state,axis=0)\n",
    "            #state = np.expand_dims(state,axis=2)\n",
    "            #print(state.shape)\n",
    "            #plt.plot(state)\n",
    "            #state = np.expand_dims(state,axis=4)\n",
    "            Q_reward_counter[e] = np.max(actvals)\n",
    "            total_reward_counter[e] = total_reward\n",
    "\n",
    "            break\n",
    "    # train the agent with the experience of the episode\n",
    "    agent.replay(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.max(Q_reward_counter))\n",
    "plt.plot(Q_reward_counter)\n",
    "plt.axis([0, Q_reward_counter.shape[0], np.min(Q_reward_counter), np.max(Q_reward_counter)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 50\n",
    "rew_avg = np.empty([episodes])\n",
    "for pnts in range(window,total_reward_counter.shape[0]):\n",
    "    rew_avg[pnts] =  np.mean(total_reward_counter[(pnts-window) : pnts+window])\n",
    "plt.plot(total_reward_counter)\n",
    "plt.plot(rew_avg)\n",
    "plt.axis([0, total_reward_counter.shape[0], np.min(rew_avg), np.max(rew_avg)])\n",
    "np.argmax(rew_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#playtest\n",
    "frms = 0\n",
    "env = gym.make('Breakout-ramDeterministic-v0')\n",
    "\n",
    "#agent = DQNAgent(env)\n",
    "#agent.model = keras.models.load_model('breakoutfeedforwardRAM1k.h5')\n",
    "\n",
    "#envisual = gym.make('BreakoutDeterministic-v0')\n",
    "state = env.reset()\n",
    "#statevisual = envisual.reset()\n",
    "\n",
    "\n",
    "\n",
    "plt.ion()\n",
    "fig = plt.figure()\n",
    "\n",
    "\n",
    "plt.axis([0, 432, 0, 288])\n",
    "agent.epsilon = 0.05;\n",
    "actgrid = [[0,0,0],[0,0,0]]\n",
    "#for e in range(10000):\n",
    "#    state = env.reset()\n",
    "#statevisual = envisual.reset()\n",
    "for time_t in range(10000):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    #state = np.expand_dims(state,axis=4)\n",
    "\n",
    "\n",
    "    actvals, action = agent.act(state)\n",
    "    state = np.expand_dims(state,axis=0)\n",
    "    expectation = np.amax(agent.model.predict(state))\n",
    "    #print(state)\n",
    "\n",
    "    #print(agent.model.predict(np.random.rand(1,128)))\n",
    "\n",
    "    #actgrid[0][0] = actvals[0][5]\n",
    "    #actgrid[0][1] = actvals[0][1]\n",
    "    #actgrid[0][2] = actvals[0][4]\n",
    "\n",
    "    actgrid[0][0] = actvals[0][3]\n",
    "    actgrid[0][1] = actvals[0][0]\n",
    "    actgrid[0][2] = actvals[0][2]\n",
    "\n",
    "    actgrid[1][0] = actvals[0][3]\n",
    "    actgrid[1][1] = actvals[0][0]\n",
    "    actgrid[1][2] = actvals[0][2]\n",
    "\n",
    "    #actgrid[2][0] = actvals[0][8]\n",
    "    #actgrid[2][1] = actvals[0][4]\n",
    "    #actgrid[2][2] = actvals[0][7]\n",
    "\n",
    "    #action = 3\n",
    "    state, reward1, done, _ = env.step(action)\n",
    "    #statevisual, reward2, done, info = envisual.step(action)\n",
    "    #state, reward, done, _ = env.step(0)\n",
    "\n",
    "    #print(actvals)\n",
    "    #print(actgrid)\n",
    "\n",
    "    #f, axarr = plt.subplots(1,2)\n",
    "\n",
    "    #cmap = colors.Colormap('magma',N = 256)\n",
    "    #norm = colors.Normalize(np.min(actgrid),np.max(actgrid),clip=True)\n",
    "    #print(actgrid)\n",
    "    #print(zactgrid)\n",
    "    #axarr[0].imshow(actgrid,vmin = np.min(actgrid), vmax = np.max(actgrid))\n",
    "    plt.imshow(actgrid,vmin = np.min(actgrid), vmax = np.max(actgrid))\n",
    "    #axarr[1].imshow(statevisual)\n",
    "    env.render()\n",
    "\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    print(action)\n",
    "    print(expectation)\n",
    "    #print(info)\n",
    "    print(done)\n",
    "    #print(reward1)\n",
    "    #print(reward2)\n",
    "    #time.sleep(0.001)\n",
    "    #print(action)\n",
    "\n",
    "\n",
    "    frms = frms+1\n",
    "\n",
    "    if done:\n",
    "        env.close()\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.model.save('breakoutfeedforwardRAM10k.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samretrogames",
   "language": "python",
   "name": "samretrogames"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
