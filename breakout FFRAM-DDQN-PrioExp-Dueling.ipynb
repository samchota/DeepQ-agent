{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import collections\n",
    "import random\n",
    "import time\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.pyplot import *\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import csv\n",
    "from gym import envs\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import BatchNormalization, Reshape, UpSampling2D, Conv2DTranspose, LeakyReLU, ZeroPadding2D, Input, Concatenate, concatenate, Lambda\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras import regularizers, optimizers\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import regularizers, optimizers\n",
    "from keras.utils import to_categorical\n",
    "#np.random.seed(1234)\n",
    "import scipy.io as sio\n",
    "import scipy as scp\n",
    "from scipy import stats\n",
    "import operator\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import keras\n",
    "import keras.losses\n",
    "from keras.models import load_model\n",
    "from baselines import deepq\n",
    "from baselines.deepq.replay_buffer import PrioritizedReplayBuffer\n",
    "from IPython.display import display, clear_output\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "#config=tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction=0.5\n",
    "#set_session(tf.Session(config=config))\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_envs = envs.registry.all()\n",
    "env_ids = [env_spec.id for env_spec in all_envs]\n",
    "#print(np.transpose(env_ids)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FF\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        #(210,160,3),6\n",
    "        self.drop = 0\n",
    "        self.state_size = (128,)\n",
    "        self.action_size = 3 # 9 for pacman, 6 for space invader\n",
    "        #self.memory = collections.deque(maxlen=1000000)\n",
    "        self.PERalphaPRIO = 0.7\n",
    "        self.memory = PrioritizedReplayBuffer(1000000,alpha = 0.7)\n",
    "        self.gamma = 0.99    # discount rate 0.95\n",
    "        self.epsilon = 1  # exploration rate 1 \n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = []\n",
    "        self.learning_rate = []\n",
    "        self.time_penalty = 0\n",
    "        self.fix = False       \n",
    "        self.model = []\n",
    "        self.target_model = []\n",
    "        self.target_update_rate = []\n",
    "        self.optimizer_param = []\n",
    "        self.frames_played = 0\n",
    "        self.frames_trained = 0\n",
    "        \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        #model = Sequential()\n",
    "        \n",
    "        input_layer = Input(shape=self.state_size, name = 'Input')\n",
    "        \n",
    "        dense1 = Dense(128, activation='relu', name = 'Dense1')(input_layer)\n",
    "                \n",
    "        dense2 = Dense(128, activation='relu', name = 'Dense2')(dense1)\n",
    "        \n",
    "        dense3 = Dense(128, activation='relu', name = 'Dense3')(dense2)\n",
    "        \n",
    "        denseLin1 = Dense(64, activation='linear', name = 'DenseAdvantage')(dense3)\n",
    "\n",
    "        advantage = Dense(self.action_size, activation='linear', name = 'Advantage')(denseLin1)\n",
    "        \n",
    "        denseLin2 = Dense(64, activation='linear', name = 'DenseValue')(dense3)\n",
    "        \n",
    "        value = Dense(1, name = 'Value')(denseLin2)\n",
    "        \n",
    "        policy = Lambda(lambda x: x[0]-K.mean(x[0])+x[1], output_shape = (self.action_size,), name = 'Policy')([advantage,value])\n",
    "        \n",
    "        model = Model(input = [input_layer], output =[policy])\n",
    "        \n",
    "        model.compile(loss=huber_loss, optimizer=self.optimizer_param)        \n",
    "        #model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        \n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def copy_model(self):\n",
    "        \n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        return []\n",
    "    \n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        #self.memory.append((state, action, reward, next_state, done, error))\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            act_values = np.zeros((1,self.action_size))\n",
    "            action = env.action_space.sample()\n",
    "            while action == 1:\n",
    "                action = env.action_space.sample()\n",
    "            return act_values, action\n",
    "        state = np.expand_dims(state,axis=0)\n",
    "        #print(state.shape)\n",
    "        act_values = self.model.predict(state)\n",
    "        action = np.argmax(act_values)\n",
    "        if action == 1:\n",
    "            action = 3\n",
    "        return act_values,action   # returns action\n",
    "\n",
    "    def replay(self, batch_size,e):\n",
    "        priorities = np.zeros(32)\n",
    "        #minibatch = random.sample(self.memory, batch_size)\n",
    "        minibatch = self.memory.sample(32,self.PERalphaPRIO)\n",
    "        idxes = minibatch[6]\n",
    "        if e%self.target_update_rate == 0:\n",
    "            self.copy_model()\n",
    "            \n",
    "        for num in range(0,32):\n",
    "            state = minibatch[0][num]\n",
    "            action = minibatch[1][num]\n",
    "            reward = minibatch[2][num]\n",
    "            next_state = minibatch[3][num]\n",
    "            done = minibatch[4][num]\n",
    "            \n",
    "            state = np.expand_dims(state,axis=0)\n",
    "            next_state = np.expand_dims(next_state,axis=0)\n",
    "              \n",
    "            target = 0 #if we are done the final reward is our target\n",
    "            if not done: #if we are not done the target is the current reward plut the predicted reward from the next state\n",
    "                \n",
    "                #Normal Q\n",
    "                #target = reward + self.gamma * np.amax(self.model.predict(next_state))\n",
    "                \n",
    "                #Double Q\n",
    "                target = reward + self.gamma * np.amax(self.target_model.predict(next_state))\n",
    "\n",
    "            target_f = self.model.predict(state) \n",
    "            #print(target_f)\n",
    "            target_f[0][action] = target - self.time_penalty\n",
    "            \n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0) \n",
    "            self.frames_trained = self.frames_trained+1\n",
    "            \n",
    "            #####update error#####\n",
    "            values = self.target_model.predict(state)\n",
    "            expected = values[0][action]\n",
    "        \n",
    "            target = reward + self.gamma * np.amax(self.target_model.predict(next_state))\n",
    "        \n",
    "            error = abs(expected-target)\n",
    "        \n",
    "            priorities[num] = (error+0.01)**self.PERalphaPRIO\n",
    "            \n",
    "            ########\n",
    "        if self.fix == False:\n",
    "           \n",
    "            self.memory.update_priorities(idxes,priorities)\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon -= self.epsilon_decay\n",
    "                \n",
    "def huber_loss(y_true, y_pred):\n",
    "    clip_delta=2.0\n",
    "    error = y_true - y_pred\n",
    "    cond  = tf.keras.backend.abs(error) < clip_delta\n",
    "\n",
    "    squared_loss = 0.5 * tf.keras.backend.square(error)\n",
    "    linear_loss  = clip_delta * (tf.keras.backend.abs(error) - 0.5 * clip_delta)\n",
    "\n",
    "    return tf.where(cond, squared_loss, linear_loss)\n",
    "keras.losses.huber_loss = huber_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent,env,episodes,iteration,iterations,fix):\n",
    "    total_reward_counter = np.empty([episodes])\n",
    "    Q_reward_counter = np.empty([episodes])\n",
    "    performance = 0\n",
    "    \n",
    "    #agent.epsilon = 1\n",
    "    for e in range(episodes):\n",
    "        \n",
    "        if e > fix: \n",
    "            agent.fix = False\n",
    "        \n",
    "        total_reward = 0\n",
    "        state = env.reset()\n",
    "        state, reward, done, info = env.step(0)\n",
    "        state = state/256\n",
    "        i_died = 0\n",
    "        lives_before = 5\n",
    "        lives_after = 4\n",
    "        \n",
    "        for time_t in range(10000):\n",
    "            agent.frames_played = agent.frames_played+1\n",
    " \n",
    "            if time_t < (i_died+2):\n",
    "                next_state, reward, done, info = env.step(1)\n",
    "                if reward>0:   #should never happen\n",
    "                    reward = 1\n",
    "                lives_before = info['ale.lives']\n",
    "                lives_after = info['ale.lives']\n",
    "                next_state = next_state/256\n",
    "            else:\n",
    "                lives_before = info['ale.lives']\n",
    "                \n",
    "                actvals, action = agent.act(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                next_state = next_state/256\n",
    "                lives_after = info['ale.lives']\n",
    "                \n",
    "                if reward>0:\n",
    "                    reward = 1\n",
    "                    total_reward = total_reward + reward   \n",
    "            \n",
    "                if action == 3: #transform environment action back to agent action\n",
    "                    action = 1\n",
    "                \n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            if lives_after<lives_before:\n",
    "                i_died = time_t\n",
    "            \n",
    "            #env.render()\n",
    "            #time.sleep(0.5)\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                clear_output(wait=True)\n",
    "                print(\"episode: {}/{}, score: {}, epsilon: {}, frames played: {},frames trained: {},performance: {}\".format(e+1, episodes, total_reward, agent.epsilon, agent.frames_played, agent.frames_trained, performance))\n",
    "                Q_reward_counter[e] = np.max(actvals)\n",
    "                total_reward_counter[e] = total_reward\n",
    "\n",
    "                break\n",
    "        if agent.fix == False:\n",
    "            agent.replay(32,e)\n",
    "            if e>1000:\n",
    "                performance = np.mean(total_reward_counter[(e-900):e])\n",
    "    return Q_reward_counter, total_reward_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "{'ale.lives': 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#env = gym.make('SpaceInvaders-v0')\n",
    "env = gym.make('Breakout-ramDeterministic-v4')\n",
    "balls = np.zeros(1000)\n",
    "for i in  range(1000):\n",
    "    balls[i] = env.action_space.sample()\n",
    "print(np.max(balls)+1)\n",
    "state = env.reset()\n",
    "next_state, reward, done, info = env.step(1)\n",
    "lives_before = info['ale.lives']\n",
    "print(info)\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 218821/300000, score: 20, epsilon: 0.04999000000192088, frames played: 89721161,frames trained: 6997408,performance: 15.457777777777778\n"
     ]
    }
   ],
   "source": [
    "fix = 150\n",
    "episodes = 300000 # 150.000\n",
    "\n",
    "iterations = 1\n",
    "\n",
    "target_update_rate = 100 #1250 #300\n",
    "target_update_rate = np.repeat(target_update_rate,iterations)\n",
    "\n",
    "learning_rates = [0.0001]#,0.0002,0.0002]\n",
    "learning_rates = np.repeat(learning_rates,iterations)\n",
    "\n",
    "decay = [0.00001]#,0.99992,0.99992]   #30.000eps\n",
    "decay = np.repeat(decay,iterations)\n",
    "\n",
    "#optim = ['rmsprop','adam','sgd']\n",
    "optim = ['adam']\n",
    "optim = np.repeat(optim,iterations)\n",
    "##################################################################\n",
    "\n",
    "total_rewards = np.empty([iterations,episodes])\n",
    "Q_rewards = np.empty([iterations,episodes])\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    env = gym.make('Breakout-ramDeterministic-v4')\n",
    "\n",
    "    agent = DQNAgent()   \n",
    "    \n",
    "    agent.fix = True\n",
    "        \n",
    "    #Decay parameter\n",
    "    agent.epsilon_decay = decay[iteration]\n",
    "    \n",
    "    agent.learning_rate = learning_rates[iteration]\n",
    "    agent.target_update_rate = target_update_rate[iteration]\n",
    "    \n",
    "    if optim[iteration] == 'rmsprop':\n",
    "        optimizer_param = optimizers.RMSprop(lr=agent.learning_rate)\n",
    "        \n",
    "    if optim[iteration] == 'adam':\n",
    "        optimizer_param = optimizers.Adam(lr=agent.learning_rate)\n",
    "        \n",
    "    if optim[iteration] == 'sgd':\n",
    "        optimizer_param = optimizers.SGD(lr=agent.learning_rate)\n",
    "        \n",
    "    agent.optimizer_param =  optimizer_param \n",
    "    \n",
    "    agent.model = agent._build_model()\n",
    "    agent.target_model = agent._build_model()\n",
    "    agent.copy_model()\n",
    "    #agent.target_model = agent._build_model()\n",
    "    Q_reward_counter, total_reward_counter = train_agent(agent,env,episodes,iteration,iterations,fix)\n",
    "    \n",
    "    total_rewards[iteration][:] = total_reward_counter\n",
    "    Q_rewards[iteration][:] = Q_reward_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterations = 20\n",
    "plt.figure(figsize=(10,5))\n",
    "a = subplot(121)\n",
    "#iterations = 10\n",
    "iteration_value = optim\n",
    "window = 1000\n",
    "\n",
    "total_avg_all = np.empty([iterations,episodes-(window*2)])\n",
    "#x = np.arange(episodes)\n",
    "for iteration in range(iterations):\n",
    "    total_avg = np.zeros([episodes-(window*2)])\n",
    "    for pnts in range(total_rewards[iteration].shape[0]-(window*2)):\n",
    "        total_avg[pnts] =  np.mean(total_rewards[iteration][(pnts-window) : pnts+window])\n",
    "\n",
    "    perf, = plt.plot(np.float32(total_avg),label = iteration_value[iteration])\n",
    "    total_avg_all[iteration] = total_avg \n",
    "#legend()\n",
    "plt.plot(np.mean(total_avg_all,0),linewidth=4.0,color='b')\n",
    "plt.axis([0, episodes, 0, 15])\n",
    "title('Total Reward')\n",
    "subplot(122)\n",
    "\n",
    "iteration_value = optim\n",
    "\n",
    "\n",
    "rew_avg_all = np.empty([iterations,episodes-(window*2)])\n",
    "#x = np.arange(episodes)\n",
    "for iteration in range(iterations):\n",
    "    rew_avg = np.zeros([episodes-(window*2)])\n",
    "    for pnts in range(Q_rewards[iteration].shape[0]-(window*2)):\n",
    "        rew_avg[pnts] =  np.mean(Q_rewards[iteration][(pnts-window) : pnts+window])\n",
    "\n",
    "    perf, = plt.plot(np.float32(rew_avg),label = iteration_value[iteration])\n",
    "    rew_avg_all[iteration] = rew_avg \n",
    "#legend()\n",
    "plt.plot(np.mean(rew_avg_all,0),linewidth=4.0,color='b')\n",
    "plt.axis([0, episodes, 0, 1])\n",
    "title('Q Values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt('total_reward1mil.csv',np.transpose(total_rewards),delimiter=\",\")\n",
    "#np.savetxt('Q_reward1mil.csv',np.transpose(Q_rewards),delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.model.save('breakoutfeedforwardRAM1milka.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #playtest\n",
    "\n",
    "# agent = DQNAgent()\n",
    "# agent.model = keras.models.load_model('breakoutfeedforwardRAM300ka.h5')\n",
    "\n",
    "# i_died = 0\n",
    "# frms = 0\n",
    "# lives_before = 5\n",
    "# lives_after = 4\n",
    "# env = gym.make('Breakout-ramDeterministic-v4')\n",
    "# action = 0\n",
    "\n",
    "\n",
    "# #model = model_from_json(open('breakoutfeedforwardRAM150k.h5').read())\n",
    "# #model.load_weights(os.path.join(os.path.dirname(modelFile), 'model_weights.h5'))\n",
    "\n",
    "# #envisual = gym.make('BreakoutDeterministic-v0')\n",
    "# state = env.reset()\n",
    "# #statevisual = envisual.reset()\n",
    "\n",
    "\n",
    "\n",
    "# plt.ion()\n",
    "# fig = plt.figure()\n",
    "\n",
    "\n",
    "# plt.axis([0, 432, 0, 288])\n",
    "# agent.epsilon = 0;\n",
    "# actgrid = [[0,0,0],[0,0,0]]\n",
    "# actvals = [[0,0,0],[0,0,0]]\n",
    "# tot_rew = 0\n",
    "# #for e in range(10000):\n",
    "# #    state = env.reset()\n",
    "# #statevisual = envisual.reset()\n",
    "# for time_t in range(10000):\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "#     #state = np.expand_dims(state,axis=4)\n",
    "\n",
    "\n",
    "#     state_pred = np.expand_dims(state,axis=0)\n",
    "#     expectation = np.amax(agent.model.predict(state_pred))\n",
    "\n",
    "#     #print(agent.model.predict(np.random.rand(1,128)))\n",
    "\n",
    "#     #actgrid[0][0] = actvals[0][5]\n",
    "#     #actgrid[0][1] = actvals[0][1]\n",
    "#     #actgrid[0][2] = actvals[0][4]\n",
    "\n",
    "#     actgrid[0][0] = actvals[0][1]\n",
    "#     actgrid[0][1] = actvals[0][0]\n",
    "#     actgrid[0][2] = actvals[0][2]\n",
    "\n",
    "#     actgrid[1][0] = actvals[0][1]\n",
    "#     actgrid[1][1] = actvals[0][0]\n",
    "#     actgrid[1][2] = actvals[0][2]\n",
    "\n",
    "#     #actgrid[2][0] = actvals[0][8]\n",
    "#     #actgrid[2][1] = actvals[0][4]\n",
    "#     #actgrid[2][2] = actvals[0][7]\n",
    "\n",
    "#     #action = 3\n",
    "    \n",
    "            \n",
    "#     if time_t < (i_died+2):\n",
    "#         next_state, reward, done, info = env.step(1)\n",
    "#         if reward>0:   #should never happen\n",
    "#             reward = 1\n",
    "#         lives_before = info['ale.lives']\n",
    "#         lives_after = info['ale.lives']\n",
    "#         next_state = next_state/256\n",
    "#     else:\n",
    "#         lives_before = info['ale.lives']\n",
    "\n",
    "#         actvals, action = agent.act(state)\n",
    "#         next_state, reward, done, info = env.step(action)\n",
    "#         next_state = next_state/256\n",
    "#         lives_after = info['ale.lives']\n",
    "\n",
    "#         if action == 3: #transform environment action back to agent action\n",
    "#             action = 1\n",
    "\n",
    "#     if lives_after<lives_before:\n",
    "#         i_died = time_t\n",
    "            \n",
    "#             #env.render()\n",
    "#             #time.sleep(0.5)\n",
    "#     state = next_state\n",
    "        \n",
    "#     tot_rew = tot_rew+reward\n",
    "\n",
    "#     env.render()\n",
    "\n",
    "#     plt.imshow(actgrid,vmin = np.min(actgrid), vmax = np.max(actgrid))\n",
    "\n",
    "\n",
    "#     plt.show()\n",
    "#     print(action)\n",
    "#     #print(expectation)\n",
    "#     #print(lives_before)\n",
    "#     #print(tot_rew)\n",
    "#     #print(frms)\n",
    "#     #print(reward2)\n",
    "#     #time.sleep(0.05)\n",
    "#     #print(action)\n",
    "\n",
    "\n",
    "#     frms = frms+1\n",
    "\n",
    "#     if done:\n",
    "#         env.close()\n",
    "#         break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.model.save('breakoutfeedforwardRAM10k.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "samretrogames",
   "language": "python",
   "name": "samretrogames"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
