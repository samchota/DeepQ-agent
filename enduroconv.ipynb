{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import collections\n",
    "import random\n",
    "import time\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import csv\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import BatchNormalization, Reshape, UpSampling2D, Conv2DTranspose, LeakyReLU, ZeroPadding2D, Input\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras import regularizers, optimizers\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import regularizers, optimizers\n",
    "from keras.utils import to_categorical\n",
    "np.random.seed(1234)\n",
    "import scipy.io as sio\n",
    "from skimage.transform import rescale, resize,downscale_local_mean\n",
    "import scipy as scp\n",
    "from scipy import stats\n",
    "import operator\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from IPython.display import display, clear_output\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= '0'\n",
    "#config=tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction=0.5\n",
    "#set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Deep Q-learning Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self,a):\n",
    "        #(210,160,3),6\n",
    "        self.drop = 0.5\n",
    "        self.state_size = (78,75,1)\n",
    "        self.action_size = 9 # 9 for pacman, 6 for space invader\n",
    "        self.memory = collections.deque(maxlen=1000000)\n",
    "        self.gamma = 0.98    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.learning_rate = 10**-4\n",
    "        self.time_penalty = 1\n",
    "        self.model = self._build_model()\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        #model.add(Dense(128, input_shape=self.state_size, activation='relu'))\n",
    "        #model.add(Dropout(self.drop))\n",
    "        #model.add(Dense(64, input_shape=self.state_size, activation='relu'))\n",
    "        #model.add(Dropout(self.drop))\n",
    "        model.add(Conv2D(16, kernel_size = (8,8), strides = (4,4), padding='valid', input_shape=self.state_size, activation='relu'))\n",
    "        model.add(Dropout(self.drop))\n",
    "      \n",
    "        model.add(Conv2D(32, kernel_size = (4,4) , strides = (2,2), padding='valid',  activation='relu'))\n",
    "        model.add(Dropout(self.drop))\n",
    "      \n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(self.drop))\n",
    "        model.add(Flatten())\n",
    "      \n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "       \n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary()\n",
    "        return model\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) #previous state, action, reward after action\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            act_values = np.zeros((1,9))\n",
    "            return act_values,env.action_space.sample()\n",
    "        state = np.expand_dims(state,axis=0)\n",
    "        #print(state.shape)\n",
    "        act_values = self.model.predict(state)\n",
    "        #return np.argmax(act_values[0])  # returns action\n",
    "        return act_values,np.argmax(act_values)   # returns action\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = np.expand_dims(state,axis=0)\n",
    "            next_state = np.expand_dims(next_state,axis=0)\n",
    "            target = reward #if we are done the final reward is our target\n",
    "            if not done: #if we are not done the target is the current reward plut the predicted reward from the next state\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)) #two frames ahead\n",
    "                #print(self.gamma * np.amax(self.model.predict(next_state)))\n",
    "            target_f = self.model.predict(state) \n",
    "            #print(target_f)\n",
    "            target_f[0][action] = target - self.time_penalty\n",
    "            #print(target_f)\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Enduro-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78, 75, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "next_state, reward, done, info = env.step(1)\n",
    "lives_before = info['ale.lives']\n",
    "#env.render()\n",
    "state = state[0:155,10:160,:]\n",
    "state_s = rescale(state,(0.5,0.5),multichannel = True,mode = 'constant',anti_aliasing = False)\n",
    "plt.imshow(state_s)\n",
    "state_s.shape\n",
    "\n",
    "#plt.imshow(state_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 18, 17, 16)        1040      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18, 17, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 7, 32)          8224      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8, 7, 256)         8448      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 14336)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 129033    \n",
      "=================================================================\n",
      "Total params: 146,745\n",
      "Trainable params: 146,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "stepz = 1000;\n",
    "reward = 0\n",
    "death_penalty = 50\n",
    "total_reward_counter = np.zeros([episodes])\n",
    "Q_reward_counter = np.zeros([episodes])\n",
    "\n",
    "# initialize gym environment and the agent\n",
    "env = gym.make('Enduro-v0')\n",
    "#env = gym.make('SpaceInvaders-v0')\n",
    "agent = DQNAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 233/1000, score: 0.0, epsilon: 0.7920612314455105\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ddb847566994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m155\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mnext_state_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmultichannel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'constant'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manti_aliasing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/samretrogames/lib/python3.6/site-packages/skimage/transform/_warps.py\u001b[0m in \u001b[0;36mrescale\u001b[0;34m(image, scale, order, mode, cval, clip, preserve_range, multichannel, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[1;32m    280\u001b[0m                   \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreserve_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                   \u001b[0manti_aliasing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manti_aliasing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m                   anti_aliasing_sigma=anti_aliasing_sigma)\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/samretrogames/lib/python3.6/site-packages/skimage/transform/_warps.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[1;32m    167\u001b[0m         out = warp(image, tform, output_shape=output_shape, order=order,\n\u001b[1;32m    168\u001b[0m                    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                    preserve_range=preserve_range)\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# n-dimensional interpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/samretrogames/lib/python3.6/site-packages/skimage/transform/_warps.py\u001b[0m in \u001b[0;36mwarp\u001b[0;34m(image, inverse_map, map_args, output_shape, order, mode, cval, clip, preserve_range)\u001b[0m\n\u001b[1;32m    848\u001b[0m                     dims.append(_warp_fast(image[..., dim], matrix,\n\u001b[1;32m    849\u001b[0m                                            \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m                                            order=order, mode=mode, cval=cval))\n\u001b[0m\u001b[1;32m    851\u001b[0m                 \u001b[0mwarped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mskimage/transform/_warps_cy.pyx\u001b[0m in \u001b[0;36mskimage.transform._warps_cy._warp_fast\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/samretrogames/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m     \"\"\"Convert the input to an array.\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate the game\n",
    "for e in range(episodes):\n",
    "    total_reward = 0\n",
    "    \n",
    "    # reset state in the beginning of each game\n",
    "    state = env.reset()\n",
    "    state, reward, done, info = env.step(0)\n",
    "    state = state[0:155,10:160,:]\n",
    "    state_s = rescale(state,(0.5,0.5),multichannel = True,mode = 'constant',anti_aliasing = False)\n",
    "    state_s = state_s[:,:,0]\n",
    "    state_s = np.expand_dims(state_s,axis=2)\n",
    "    \n",
    "\n",
    "    for time_t in range(stepz):\n",
    "        \n",
    "        reward_cum = 0\n",
    "        \n",
    "        actvals, action = agent.act(state_s)\n",
    "        if e < 32:\n",
    "            action = 1\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        reward_cum += reward\n",
    "        #next_state, reward, done, info = env.step(action)\n",
    "        #reward_cum += reward\n",
    "        #next_state, reward, done, info = env.step(action)\n",
    "        #reward_cum += reward\n",
    "        \n",
    "        next_state = next_state[0:155,10:160,:]\n",
    "        next_state_s = rescale(next_state,(0.5,0.5),multichannel = True,mode = 'constant',anti_aliasing = False)\n",
    "\n",
    "        \n",
    "        lives_after = info['ale.lives']\n",
    "        #if reward_cum>0:\n",
    "        #    reward_cum = 10\n",
    "        #if lives_after < lives_before:\n",
    "        #    reward_cum = -death_penalty\n",
    "            \n",
    "        next_state_s = next_state_s[:,:,0]\n",
    "        next_state_s = np.expand_dims(next_state_s,axis=2)\n",
    "        #print(next_state.shape)\n",
    "        total_reward = total_reward + reward_cum\n",
    "\n",
    "        agent.remember(state_s, action, reward, next_state_s, done)\n",
    "        lives_before = lives_after\n",
    "        state_s = next_state_s\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    print(\"episode: {}/{}, score: {}, epsilon: {}\".format(e, episodes, total_reward, agent.epsilon))\n",
    "                        \n",
    "    Q_reward_counter[e] = np.max(actvals)\n",
    "    total_reward_counter[e] = total_reward\n",
    "       \n",
    "  \n",
    "    agent.replay(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Q_reward_counter)\n",
    "plt.axis([0, Q_reward_counter.shape[0], 0, np.max(Q_reward_counter)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 20\n",
    "rew_avg = np.empty([episodes])\n",
    "for pnts in range(window,total_reward_counter.shape[0]):\n",
    "    rew_avg[pnts] =  np.mean(total_reward_counter[(pnts-window) : pnts+window])\n",
    "plt.plot(total_reward_counter)\n",
    "plt.plot(rew_avg)\n",
    "plt.axis([0, Q_reward_counter.shape[0], 0, np.max(total_reward_counter)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#playtest\n",
    "state = env.reset()\n",
    "state = state[0:155,10:160,:]\n",
    "\n",
    "state_s = rescale(state,(0.5,0.5),multichannel = True,mode = 'constant',anti_aliasing = False)\n",
    "state_vis = state\n",
    "state_s = state_s[:,:,0]\n",
    "plt.ion()\n",
    "fig = plt.figure()\n",
    "\n",
    "#plt.axis([0, 432, 0, 288])\n",
    "agent.epsilon = 1;\n",
    "actgrid = [[0,0,0],[0,0,0],[0,0,0]]\n",
    "for time_t in range(stepz):\n",
    "    print(time_t)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    state_s = np.expand_dims(state_s,axis=2)\n",
    "    #print(state_s.shape)\n",
    "    actvals, action = agent.act(state_s)\n",
    "    \n",
    "    actgrid[0][0] = actvals[0][6]\n",
    "    actgrid[0][1] = actvals[0][1]\n",
    "    actgrid[0][2] = actvals[0][5]\n",
    "    \n",
    "    actgrid[1][0] = actvals[0][3]\n",
    "    actgrid[1][1] = actvals[0][0]\n",
    "    actgrid[1][2] = actvals[0][2]\n",
    "    \n",
    "    actgrid[2][0] = actvals[0][8]\n",
    "    actgrid[2][1] = actvals[0][4]\n",
    "    actgrid[2][2] = actvals[0][7]\n",
    "    \n",
    "    f, axarr = plt.subplots(1,2)\n",
    "    axarr[0].imshow(state_vis)\n",
    "\n",
    "    axarr[1].imshow(actgrid,vmin = np.min(actgrid), vmax = np.max(actgrid))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    time.sleep(0.005)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    print(reward)\n",
    "    print(action)\n",
    "    #state, reward, done, _ = env.step(0)\n",
    "    #state, reward, done, _ = env.step(0)\n",
    "    state = state[0:155,10:160,:]\n",
    "    state_s = rescale(state,(0.5,0.5),multichannel = True,mode = 'constant',anti_aliasing = False)\n",
    "    state_vis = state\n",
    "    state_s = state_s[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samretrogames",
   "language": "python",
   "name": "samretrogames"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
